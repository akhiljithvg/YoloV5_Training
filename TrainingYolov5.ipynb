{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhiljithvg/YoloV5_Training/blob/main/TrainingYolov5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **YOLOv5n Training Notebook**\n",
        "Welcome to this YOLOv5n (Nano) training notebook! YOLOv5 is one of the most popular object detection models, known for its speed and efficiency. The `YOLOv5n` variant is the smallest and fastest version, making it ideal for real-time applications and edge devices with limited resources.\n",
        "\n",
        "### **What You'll Learn in This Notebook**\n",
        "This notebook provides a step-by-step guide to:\n",
        "- Setting up the YOLOv5 environment.\n",
        "- Preparing and annotating your dataset.\n",
        "- Training the YOLOv5n model for object detection tasks.\n",
        "- Evaluating the model's performance on a custom dataset.\n",
        "\n",
        "### **Why YOLOv5n?**\n",
        "YOLOv5n is specifically designed to:\n",
        "- Be lightweight for faster inference on devices with low computational power.\n",
        "- Deliver competitive accuracy for object detection in real-world applications.\n",
        "\n",
        "### **Use Cases**\n",
        "YOLOv5n is suitable for various tasks, such as:\n",
        "- Real-time object detection on mobile or IoT devices.\n",
        "- Applications in robotics, surveillance, and embedded systems.\n",
        "- Quick prototyping and experimentation.\n",
        "\n",
        "### **Let's Get Started!**\n",
        "Follow the instructions in each section to train your YOLOv5n model. By the end of this notebook, you'll have a trained object detection model ready for deployment.\n"
      ],
      "metadata": {
        "id": "NO9jY9YICIzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Dataset Preparation**\n",
        "Before training your YOLOv5n model, you need a properly annotated dataset. Object detection models require labeled data in the YOLO format, which includes the bounding box coordinates and class labels for each object in an image. Below, we’ll walk you through the steps to prepare your dataset.\n",
        "\n",
        "#### **Step 1: Annotate Your Images**\n",
        "You can use one of the following tools to annotate your images:\n",
        "\n",
        "1. **[Roboflow](https://roboflow.com/):**  \n",
        "   - An intuitive, web-based tool for managing and annotating datasets.  \n",
        "   - Supports various formats and automatically converts your annotations into YOLO format.  \n",
        "   - Provides options for augmenting your dataset with transformations like rotation, flipping, or noise addition.  \n",
        "\n",
        "2. **[LabelImg](https://github.com/heartexlabs/labelImg):**  \n",
        "   - A free, open-source tool for local image annotation.  \n",
        "   - Install it on your system, load your images, and draw bounding boxes to label objects manually.  \n",
        "   - Save annotations directly in YOLO format.  \n",
        "\n",
        "#### **Step 2: Organize Your Dataset**\n",
        "Once you’ve annotated your images, organize your dataset in the following structure:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── train/   # Training images\n",
        "│   ├── val/     # Validation images\n",
        "│   └── test/    # Testing images (optional)\n",
        "├── labels/\n",
        "│   ├── train/   # YOLO-format labels for training images\n",
        "│   ├── val/     # YOLO-format labels for validation images\n",
        "│   └── test/    # YOLO-format labels for testing images (optional)\n",
        "```\n",
        "\n",
        "- **Images Folder:** Contains all the images grouped into `train`, `val`, and optionally `test` subsets.\n",
        "- **Labels Folder:** Contains corresponding `.txt` files for each image, storing the bounding box annotations in YOLO format.\n",
        "- **`data.yaml` File:** Specifies the dataset configuration, including paths to the images, number of classes, and class names.\n",
        "\n",
        "#### **YOLO Format for Annotations**\n",
        "Each `.txt` file in the `labels` folder should follow this format for bounding boxes:  \n",
        "`<class_id> <x_center> <y_center> <width> <height>`  \n",
        "\n",
        "- `class_id`: The numeric ID for the object class (starting from 0).  \n",
        "- `x_center` and `y_center`: The normalized center coordinates of the bounding box (values between 0 and 1).  \n",
        "- `width` and `height`: The normalized dimensions of the bounding box (values between 0 and 1).  \n",
        "\n",
        "#### **Step 3: Upload Your Dataset**\n",
        "After organizing the dataset:\n",
        "- If you’re using Roboflow, you can download the dataset in YOLO format and directly upload it to your Colab environment.  \n",
        "- Alternatively, upload your local dataset (annotated with LabelImg) to Colab.\n",
        "\n",
        "---\n",
        "\n",
        "### **Next Steps**\n",
        "Once your dataset is prepared, upload it to this notebook and proceed to the training section. Make sure the dataset follows the architecture outlined above to ensure compatibility with YOLOv5n."
      ],
      "metadata": {
        "id": "_ULlxtOmDIV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👇This code sets up the YOLOv5 environment in your Google Colab notebook."
      ],
      "metadata": {
        "id": "jjddODjztB1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6nGQCN534ay"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # Clone the YOLOv5 repo\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt  # Install dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👇This code connects your Google Drive to your Colab notebook. It lets you access files from your Drive directly in Colab."
      ],
      "metadata": {
        "id": "XKnk9Ykus-V5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaHMtuegGzDq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 👇This code unzips a dataset stored as a zip file in your Google Drive within your Google Colab environment."
      ],
      "metadata": {
        "id": "MYC2AhBeuWj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJEVmpSMtiOo"
      },
      "outputs": [],
      "source": [
        "zip_path = '/content/drive/My Drive/Colab Notebooks/dataset.zip'  # Replace with your zip file's path\n",
        "\n",
        "# Destination folder to extract the contents\n",
        "extract_path = '/content/drive/My Drive/Colab Notebooks'  # Folder where the data will be extracted\n",
        "\n",
        "# Unzip the file\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Dataset extracted to {extract_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👇This code lists the contents (files and subdirectories) of the specified directory within your Google Drive in Colab."
      ],
      "metadata": {
        "id": "0Zq1LMMPuj73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGW0zb3AH3PH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.listdir('/content/drive/My Drive/Colab Notebooks/dataset')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👇This code verifies the structure of your dataset directory by listing its contents."
      ],
      "metadata": {
        "id": "KD7FPsOYu6Iy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUojFsdMKGvg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dataset_dir = '/content/drive/My Drive/Colab Notebooks/dataset'  # Update this path to match where your dataset is located\n",
        "\n",
        "# Verify the dataset structure\n",
        "os.listdir(dataset_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👇This code verifies the structure of your dataset directory, specifically the 'images' and 'labels' folders, by listing their contents."
      ],
      "metadata": {
        "id": "bTX5pxGPvNtS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcvHIrQzKdCl"
      },
      "outputs": [],
      "source": [
        "# Verify the structure of the 'images' folder\n",
        "images_dir = os.path.join(dataset_dir, 'images')\n",
        "print(\"Images directory structure:\")\n",
        "print(\"Train:\", os.listdir(os.path.join(images_dir, 'train')))\n",
        "print(\"Val:\", os.listdir(os.path.join(images_dir, 'val')))\n",
        "\n",
        "# Verify the structure of the 'labels' folder\n",
        "labels_dir = os.path.join(dataset_dir, 'labels')\n",
        "print(\"Labels directory structure:\")\n",
        "print(\"Train:\", os.listdir(os.path.join(labels_dir, 'train')))\n",
        "print(\"Val:\", os.listdir(os.path.join(labels_dir, 'val')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👇This code creates a dataset.yaml file, which is a configuration file used by YOLOv5 to define the dataset for training."
      ],
      "metadata": {
        "id": "piegrO8EvbQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_087I8JKwOv"
      },
      "outputs": [],
      "source": [
        "# Create dataset.yaml file\n",
        "dataset_yaml_path = '/content/yolov5/dataset.yaml'\n",
        "\n",
        "with open(dataset_yaml_path, 'w') as f:\n",
        "    f.write('train: /content/drive/My Drive/Colab Notebooks/dataset/images/train\\n')\n",
        "    f.write('val: /content/drive/My Drive/Colab Notebooks/dataset/images/val\\n')\n",
        "    f.write('nc: 7\\n')  # Number of classes (adjust this based on your dataset)\n",
        "    f.write(\"names: ['green', 'left', 'parking', 'red', 'right', 'stop', 'yellow']\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👇This code initiates the training process for a YOLOv5n object detection model using your custom dataset and then downloads the trained weights."
      ],
      "metadata": {
        "id": "AF750vvNvylG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDn6kf4OKzKr",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!python /content/yolov5/train.py --img 320 --batch 32 --epochs 200 --data /content/yolov5/dataset.yaml --weights yolov5n.pt --device 0\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/yolov5/runs/train/exp/weights/best.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👇This code downloads a specific trained model weights file (best.pt) from your Google Colab environment to your local machine."
      ],
      "metadata": {
        "id": "kH3aBpAHv_np"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MerUcsm5OTKZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/yolov5/runs/train/exp2/weights/best.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🎉 Conclusion**\n",
        "Congratulations! You’ve successfully trained your YOLOv5n model and obtained the `best.pt` file, which represents the best-performing weights from your training process.\n",
        "\n",
        "### **What You’ve Accomplished**\n",
        "- You’ve trained a lightweight and efficient YOLOv5n model for object detection.  \n",
        "- Your model is ready to detect objects on new, unseen data based on the classes it was trained on.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Next Steps**\n",
        "1. **Model Evaluation**\n",
        "   - Test your `best.pt` model on images or videos to evaluate its real-world performance:\n",
        "     ```bash\n",
        "     !python detect.py --weights best.pt --img 640 --source <image_or_video_path>\n",
        "     ```\n",
        "     Replace `<image_or_video_path>` with the path to your test image, folder, or video.\n",
        "\n",
        "2. **Fine-Tuning**\n",
        "   - If the results are not satisfactory, consider:\n",
        "     - Adding more training data.\n",
        "     - Adjusting hyperparameters (like learning rate, batch size, etc.).\n",
        "     - Experimenting with data augmentation techniques.\n",
        "\n",
        "3. **Deployment**\n",
        "   - Deploy your model for real-time applications on edge devices like Raspberry Pi, Jetson Nano, or any hardware of your choice.  \n",
        "   - For optimized inference, export your model to formats like ONNX or TensorRT:\n",
        "     ```bash\n",
        "     !python export.py --weights best.pt --include onnx\n",
        "     ```\n",
        "\n",
        "4. **Sharing Your Model**\n",
        "   - Share the `best.pt` file or deploy it in production environments.\n",
        "   - Use platforms like [Roboflow](https://roboflow.com/) or [Hugging Face](https://huggingface.co/) for easy sharing.  \n",
        "   - You can also deploy it via a web app (e.g., Flask/Django) or integrate it into your own application.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Note**\n",
        "The `best.pt` file represents all the effort put into preparing data, annotating it, and training your model. Continue experimenting with new datasets, improving your annotations, and refining your training pipeline for even better results.\n",
        "\n"
      ],
      "metadata": {
        "id": "ajnFbOOlzA_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "                                            Thank you for using this notebook! Created by\n",
        "             \n",
        "                       \n",
        "                                                        Akhiljith Gigi ❤️\n",
        "\n",
        "\n",
        "                                                        "
      ],
      "metadata": {
        "id": "Qhell90dwzyL"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}